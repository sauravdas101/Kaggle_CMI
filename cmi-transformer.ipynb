{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":102335,"databundleVersionId":12518947,"sourceType":"competition"},{"sourceId":242954653,"sourceType":"kernelVersion"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T21:01:21.726537Z","iopub.execute_input":"2025-07-19T21:01:21.726752Z","iopub.status.idle":"2025-07-19T21:01:22.053765Z","shell.execute_reply.started":"2025-07-19T21:01:21.726733Z","shell.execute_reply":"2025-07-19T21:01:22.053036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os, json, joblib, numpy as np, pandas as pd\nfrom pathlib import Path\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\n\nfrom tensorflow.keras.utils import Sequence, to_categorical, pad_sequences\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.layers import (\n    Input, Conv1D, BatchNormalization, Activation, add, MaxPooling1D, Dropout,\n    Bidirectional, LSTM, GlobalAveragePooling1D, Dense, Multiply, Reshape,\n    Lambda, Concatenate, GRU, GaussianNoise\n)\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nimport polars as pl\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom scipy.spatial.transform import Rotation as R","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T21:01:22.055023Z","iopub.execute_input":"2025-07-19T21:01:22.055321Z","iopub.status.idle":"2025-07-19T21:01:35.578620Z","shell.execute_reply.started":"2025-07-19T21:01:22.055303Z","shell.execute_reply":"2025-07-19T21:01:35.578061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    tf.experimental.numpy.random.seed(seed)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\nseed_everything(seed=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T21:01:35.579364Z","iopub.execute_input":"2025-07-19T21:01:35.579840Z","iopub.status.idle":"2025-07-19T21:01:35.584441Z","shell.execute_reply.started":"2025-07-19T21:01:35.579802Z","shell.execute_reply":"2025-07-19T21:01:35.583711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# (Competition metric will only be imported when TRAINing)\nTRAIN = True                  # ← set to True when you want to train\nRAW_DIR = Path(\"/kaggle/input/cmi-detect-behavior-with-sensor-data\")\nPRETRAINED_DIR = Path(\"/kaggle/input/quit-diff2\")  # used when TRAIN=False\nEXPORT_DIR = Path(\"./\")                                    # artefacts will be saved here\nBATCH_SIZE = 64\nPAD_PERCENTILE = 95\nLR_INIT = 5e-4\nWD = 3e-3\nMIXUP_ALPHA = 0.4\nEPOCHS = 160\nPATIENCE = 40\n\n\nprint(\"▶ imports ready · tensorflow\", tf.__version__)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T21:01:35.585208Z","iopub.execute_input":"2025-07-19T21:01:35.585404Z","iopub.status.idle":"2025-07-19T21:01:35.651680Z","shell.execute_reply.started":"2025-07-19T21:01:35.585389Z","shell.execute_reply":"2025-07-19T21:01:35.651081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Normalizes and cleans the time series sequence. \n\ndef preprocess_sequence(df_seq: pd.DataFrame, feature_cols: list[str], scaler: StandardScaler):\n    mat = df_seq[feature_cols].ffill().bfill().fillna(0).values\n    return scaler.transform(mat).astype('float32')\n\n# MixUp the data argumentation in order to regularize the neural network. \n\nclass MixupGenerator(Sequence):\n    def __init__(self, X, y, batch_size, alpha=0.2):\n        self.X, self.y = X, y\n        self.batch = batch_size\n        self.alpha = alpha\n        self.indices = np.arange(len(X))\n    def __len__(self):\n        return int(np.ceil(len(self.X) / self.batch))\n    def __getitem__(self, i):\n        idx = self.indices[i*self.batch:(i+1)*self.batch]\n        Xb, yb = self.X[idx], self.y[idx]\n        lam = np.random.beta(self.alpha, self.alpha)\n        perm = np.random.permutation(len(Xb))\n        X_mix = lam * Xb + (1-lam) * Xb[perm]\n        y_mix = lam * yb + (1-lam) * yb[perm]\n        return X_mix, y_mix\n    def on_epoch_end(self):\n        np.random.shuffle(self.indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T21:01:35.653299Z","iopub.execute_input":"2025-07-19T21:01:35.653757Z","iopub.status.idle":"2025-07-19T21:01:35.668166Z","shell.execute_reply.started":"2025-07-19T21:01:35.653738Z","shell.execute_reply":"2025-07-19T21:01:35.667490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def remove_gravity_from_acc(acc_data, rot_data):\n\n    if isinstance(acc_data, pd.DataFrame):\n        acc_values = acc_data[['acc_x', 'acc_y', 'acc_z']].values\n    else:\n        acc_values = acc_data\n\n    if isinstance(rot_data, pd.DataFrame):\n        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    else:\n        quat_values = rot_data\n\n    num_samples = acc_values.shape[0]\n    linear_accel = np.zeros_like(acc_values)\n    \n    gravity_world = np.array([0, 0, 9.81])\n\n    for i in range(num_samples):\n        if np.all(np.isnan(quat_values[i])) or np.all(np.isclose(quat_values[i], 0)):\n            linear_accel[i, :] = acc_values[i, :] \n            continue\n\n        try:\n            rotation = R.from_quat(quat_values[i])\n            gravity_sensor_frame = rotation.apply(gravity_world, inverse=True)\n            linear_accel[i, :] = acc_values[i, :] - gravity_sensor_frame\n        except ValueError:\n             linear_accel[i, :] = acc_values[i, :]\n             \n    return linear_accel\n\ndef calculate_angular_velocity_from_quat(rot_data, time_delta=1/200): # Assuming 200Hz sampling rate\n    if isinstance(rot_data, pd.DataFrame):\n        quat_values = rot_data[['rot_x', 'rot_y', 'rot_z', 'rot_w']].values\n    else:\n        quat_values = rot_data\n\n    num_samples = quat_values.shape[0]\n    angular_vel = np.zeros((num_samples, 3))\n\n    for i in range(num_samples - 1):\n        q_t = quat_values[i]\n        q_t_plus_dt = quat_values[i+1]\n\n        if np.all(np.isnan(q_t)) or np.all(np.isclose(q_t, 0)) or \\\n           np.all(np.isnan(q_t_plus_dt)) or np.all(np.isclose(q_t_plus_dt, 0)):\n            continue\n\n        try:\n            rot_t = R.from_quat(q_t)\n            rot_t_plus_dt = R.from_quat(q_t_plus_dt)\n\n            # Calculate the relative rotation\n            delta_rot = rot_t.inv() * rot_t_plus_dt\n            \n            # Convert delta rotation to angular velocity vector\n            # The rotation vector (Euler axis * angle) scaled by 1/dt\n            # is a good approximation for small delta_rot\n            angular_vel[i, :] = delta_rot.as_rotvec() / time_delta\n        except ValueError:\n            # If quaternion is invalid, angular velocity remains zero\n            pass\n            \n    return angular_vel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T21:01:35.668799Z","iopub.execute_input":"2025-07-19T21:01:35.669025Z","iopub.status.idle":"2025-07-19T21:01:35.683823Z","shell.execute_reply.started":"2025-07-19T21:01:35.669005Z","shell.execute_reply":"2025-07-19T21:01:35.683192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\n\nclass PositionalEncoding(Layer):\n    def __init__(self, seq_len, d_model):\n        super().__init__()\n        self.seq_len = seq_len\n        self.d_model = d_model\n        self.pos_encoding = self._positional_encoding(seq_len, d_model)\n\n    def _get_angles(self, position, i, d_model):\n        angle_rates = 1 / tf.pow(10000.0, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n        return position * angle_rates\n\n    def _positional_encoding(self, seq_len, d_model):\n        position = tf.cast(tf.range(seq_len)[:, tf.newaxis], dtype=tf.float32)\n        i = tf.cast(tf.range(d_model)[tf.newaxis, :], dtype=tf.float32)\n        angle_rads = self._get_angles(position, i, d_model)\n\n        # Apply sin to even indices and cos to odd indices\n        sines = tf.math.sin(angle_rads[:, 0::2])\n        cosines = tf.math.cos(angle_rads[:, 1::2])\n\n        # Interleave sines and cosines\n        pos_encoding = tf.concat([sines, cosines], axis=-1)\n        pos_encoding = tf.reshape(pos_encoding, [1, seq_len, d_model])\n        return tf.cast(pos_encoding, tf.float32)\n\n    def call(self, x):\n        return x + self.pos_encoding[:, :tf.shape(x)[1], :]\n        \nclass LearnedPositionalEmbedding(Layer):\n    def __init__(self, seq_len, d_model):\n        super().__init__()\n        self.pos_emb = self.add_weight(\n            name=\"pos_emb\",  # ✅ explicitly name this\n            shape=[seq_len, d_model],\n            initializer=\"random_normal\"\n        )\n\n    def call(self, x):\n        return x + self.pos_emb[tf.newaxis, :, :]\n\ndef transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.3):\n    x = LayerNormalization(epsilon=1e-6)(inputs)\n    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n    x = Dropout(dropout)(x)\n    x = Add()([x, inputs])\n\n    x_skip = x\n    x = LayerNormalization(epsilon=1e-6)(x)\n    x = Dense(ff_dim, activation=\"relu\")(x)\n    x = Dropout(dropout)(x)\n    x = Dense(inputs.shape[-1])(x)\n    x = Add()([x, x_skip])\n    return x\n\ndef build_transformer_model(input_shape, n_classes, num_layers=4, head_size=64, num_heads=2, ff_dim=128, dropout=0.1):\n    inputs = Input(shape=input_shape)\n\n    x = Dense(128)(inputs)  # Linear projection to embed dimension\n    x = GaussianNoise(0.09)(x)\n    #x = PositionalEncoding(seq_len=input_shape[0], d_model=128)(x)\n    x = LearnedPositionalEmbedding(input_shape[0], 128)(x)\n\n    for _ in range(num_layers):\n        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n\n    x = GlobalAveragePooling1D()(x)\n    x = Dropout(0.3)(x)\n    x = Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n    x = Dropout(0.3)(x)\n    outputs = Dense(n_classes, activation='softmax')(x)\n\n    return Model(inputs, outputs)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T21:29:22.414504Z","iopub.execute_input":"2025-07-19T21:29:22.415095Z","iopub.status.idle":"2025-07-19T21:29:22.426804Z","shell.execute_reply.started":"2025-07-19T21:29:22.415072Z","shell.execute_reply":"2025-07-19T21:29:22.425985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_tof_model(pad_len, tof_dim, n_classes, wd=1e-4):\n    inp = Input(shape=(pad_len, tof_dim))\n\n    x = Conv1D(64, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(inp)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling1D(2)(x)\n    x = Dropout(0.2)(x)\n\n    x = Conv1D(128, 3, padding='same', use_bias=False, kernel_regularizer=l2(wd))(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x = MaxPooling1D(2)(x)\n    x = Dropout(0.2)(x)\n\n    xa = Bidirectional(LSTM(128, return_sequences=True, kernel_regularizer=l2(wd)))(x)\n    xb = Bidirectional(GRU(128, return_sequences=True, kernel_regularizer=l2(wd)))(x)\n    xc = GaussianNoise(0.09)(x)\n    xc = Dense(16, activation='elu')(xc)\n\n    x = Concatenate()([xa, xb, xc])\n    x = Dropout(0.4)(x)\n    x = attention_layer(x)\n\n    for units, drop in [(256, 0.5), (128, 0.3)]:\n        x = Dense(units, use_bias=False, kernel_regularizer=l2(wd))(x)\n        x = BatchNormalization()(x)\n        x = Activation('relu')(x)\n        x = Dropout(drop)(x)\n\n    out = Dense(n_classes, activation='softmax', kernel_regularizer=l2(wd))(x)\n    return Model(inp, out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T21:01:35.705335Z","iopub.execute_input":"2025-07-19T21:01:35.705607Z","iopub.status.idle":"2025-07-19T21:01:35.722339Z","shell.execute_reply.started":"2025-07-19T21:01:35.705591Z","shell.execute_reply":"2025-07-19T21:01:35.721761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"▶ TRAIN MODE – loading dataset …\")\ndf = pd.read_csv(RAW_DIR / \"train.csv\")\n\ntrain_dem_df = pd.read_csv(RAW_DIR / \"train_demographics.csv\")\n\n# Keep only necessary demographic columns (used later for centripetal calculations)\nneeded_dem_cols = ['subject', 'shoulder_to_wrist_cm', 'elbow_to_wrist_cm']\ntrain_dem_df = train_dem_df[needed_dem_cols]\n\n# Merge only required demographic info into a minimal df\ndf_for_groups = pd.merge(df[['sequence_id', 'subject']].drop_duplicates(), train_dem_df, on='subject', how='left')\n\n# Create lookup maps for memory-efficient access later\nshoulder_map = df_for_groups.set_index('sequence_id')['shoulder_to_wrist_cm'].to_dict()\nelbow_map = df_for_groups.set_index('sequence_id')['elbow_to_wrist_cm'].to_dict()\n\n# Clean up memory\ndel train_dem_df\ndel df_for_groups\n\n# Label encoding\nle = LabelEncoder()\ndf['gesture_int'] = le.fit_transform(df['gesture'])\nnp.save(EXPORT_DIR / \"gesture_classes.npy\", le.classes_)\ngesture_classes = le.classes_\n\n\nprint(\"  Calculating base engineered IMU features (magnitude, angle)...\")\ndf['acc_mag'] = np.sqrt(df['acc_x']**2 + df['acc_y']**2 + df['acc_z']**2)\ndf['rot_angle'] = 2 * np.arccos(df['rot_w'].clip(-1, 1))\n\nprint(\"  Calculating engineered IMU derivatives (jerk, angular velocity) for original acc_mag...\")\ndf['acc_mag_jerk'] = df.groupby('sequence_id')['acc_mag'].diff().fillna(0)\ndf['rot_angle_vel'] = df.groupby('sequence_id')['rot_angle'].diff().fillna(0)\n\nprint(\"  Removing gravity and calculating linear acceleration features...\")\n\nlinear_accel_list = []\nfor _, group in df.groupby('sequence_id'):\n    acc_data_group = group[['acc_x', 'acc_y', 'acc_z']]\n    rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n    linear_accel_group = remove_gravity_from_acc(acc_data_group, rot_data_group)\n    linear_accel_list.append(pd.DataFrame(linear_accel_group, columns=['linear_acc_x', 'linear_acc_y', 'linear_acc_z'], index=group.index))\n\ndf_linear_accel = pd.concat(linear_accel_list)\ndf = pd.concat([df, df_linear_accel], axis=1)\n\ndf['linear_acc_mag'] = np.sqrt(df['linear_acc_x']**2 + df['linear_acc_y']**2 + df['linear_acc_z']**2)\ndf['linear_acc_mag_jerk'] = df.groupby('sequence_id')['linear_acc_mag'].diff().fillna(0)\n\n\nprint(\"  Calculating angular velocity from quaternion derivatives...\")\nangular_vel_list = []\n\nfor seq_id, group in df.groupby('sequence_id'):\n    rot_data_group = group[['rot_x', 'rot_y', 'rot_z', 'rot_w']]\n    angular_vel_group = calculate_angular_velocity_from_quat(rot_data_group)\n    angular_vel_list.append(pd.DataFrame(angular_vel_group, columns=['angular_vel_x', 'angular_vel_y', 'angular_vel_z'], index=group.index))\n\n\n\ndf_angular_vel = pd.concat(angular_vel_list, axis=0)\n\ndf = pd.concat([df, df_angular_vel], axis=1)\n\n\nprint(\"  Calculating angular jerk from angular velocity...\")\ndf['angular_jerk_x'] = df.groupby('sequence_id')['angular_vel_x'].diff().fillna(0)\ndf['angular_jerk_y'] = df.groupby('sequence_id')['angular_vel_y'].diff().fillna(0)\ndf['angular_jerk_z'] = df.groupby('sequence_id')['angular_vel_z'].diff().fillna(0)\n\nprint(\"  Calculating angular snap from angular jerk...\")\ndf['angular_snap_x'] = df.groupby('sequence_id')['angular_jerk_x'].diff().fillna(0)\ndf['angular_snap_y'] = df.groupby('sequence_id')['angular_jerk_y'].diff().fillna(0)\ndf['angular_snap_z'] = df.groupby('sequence_id')['angular_jerk_z'].diff().fillna(0)\n\nmeta_cols = { } # This was an empty dict in your provided code, keeping it as is.\n\nimu_cols_base = ['linear_acc_x', 'linear_acc_y', 'linear_acc_z']\nimu_cols_base.extend([c for c in df.columns if c.startswith('rot_') and c not in ['rot_angle', 'rot_angle_vel']])\n\nimu_engineered_features = [\n    'acc_mag', 'rot_angle',\n    'acc_mag_jerk', 'rot_angle_vel',\n    'linear_acc_mag', 'linear_acc_mag_jerk',\n    'angular_vel_x', 'angular_vel_y', 'angular_vel_z',\n    'angular_jerk_x', 'angular_jerk_y', 'angular_jerk_z',\n    'angular_snap_x', 'angular_snap_y', 'angular_snap_z'# Added new angular snap features\n]\n\nimu_cols = imu_cols_base + imu_engineered_features\nimu_cols = list(dict.fromkeys(imu_cols))\n\nthm_cols_original = [c for c in df.columns if c.startswith('thm_')]\n    \ntof_aggregated_cols_template = []\nfor i in range(1, 6):\n    tof_aggregated_cols_template.extend([f'tof_{i}_mean', f'tof_{i}_std', f'tof_{i}_min', f'tof_{i}_max'])\n\nfinal_feature_cols = imu_cols + thm_cols_original + tof_aggregated_cols_template\nimu_dim_final = len(imu_cols)\ntof_thm_aggregated_dim_final = len(thm_cols_original) + len(tof_aggregated_cols_template)\n\nprint(f\"  IMU (incl. engineered & derivatives) {imu_dim_final} | THM + Aggregated TOF {tof_thm_aggregated_dim_final} | total {len(final_feature_cols)} features\")\nnp.save(EXPORT_DIR / \"feature_cols.npy\", np.array(final_feature_cols))\n\nprint(\"  Building sequences with aggregated TOF and preparing data for scaler...\")\nseq_gp = df.groupby('sequence_id') \n\nall_steps_for_scaler_list = []\nX_list_unscaled, y_list_int_for_stratify, lens = [], [], [] \n\nfor seq_id, seq_df_orig in seq_gp:\n    seq_df = seq_df_orig.copy()\n\n    for i in range(1, 6):\n        pixel_cols_tof = [f\"tof_{i}_v{p}\" for p in range(64)]\n        tof_sensor_data = seq_df[pixel_cols_tof].replace(-1, np.nan)\n        seq_df[f'tof_{i}_mean'] = tof_sensor_data.mean(axis=1)\n        seq_df[f'tof_{i}_std']  = tof_sensor_data.std(axis=1)\n        seq_df[f'tof_{i}_min']  = tof_sensor_data.min(axis=1)\n        seq_df[f'tof_{i}_max']  = tof_sensor_data.max(axis=1)\n    \n    mat_unscaled = seq_df[final_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n    \n    all_steps_for_scaler_list.append(mat_unscaled)\n    X_list_unscaled.append(mat_unscaled)\n    y_list_int_for_stratify.append(seq_df['gesture_int'].iloc[0])\n    lens.append(len(mat_unscaled))\n\nprint(\"  Fitting StandardScaler...\")\nall_steps_concatenated = np.concatenate(all_steps_for_scaler_list, axis=0)\nscaler = StandardScaler().fit(all_steps_concatenated)\njoblib.dump(scaler, EXPORT_DIR / \"scaler.pkl\")\ndel all_steps_for_scaler_list, all_steps_concatenated\n\nprint(\"  Scaling and padding sequences...\")\nX_scaled_list = [scaler.transform(x_seq) for x_seq in X_list_unscaled]\ndel X_list_unscaled\n\npad_len = int(np.percentile(lens, PAD_PERCENTILE))\n\nX = pad_sequences(X_scaled_list, maxlen=pad_len, padding='post', truncating='post', dtype='float32')\ndel X_scaled_list\n\ny_int_for_stratify = np.array(y_list_int_for_stratify)\ny = to_categorical(y_int_for_stratify, num_classes=len(le.classes_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T21:09:28.532466Z","iopub.execute_input":"2025-07-19T21:09:28.532748Z","iopub.status.idle":"2025-07-19T21:13:46.488415Z","shell.execute_reply.started":"2025-07-19T21:09:28.532729Z","shell.execute_reply":"2025-07-19T21:13:46.487529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"  Splitting data and preparing for training...\")\nX_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=82, stratify=y_int_for_stratify)\n\ncw_vals = compute_class_weight('balanced', classes=np.arange(len(le.classes_)), y=y_int_for_stratify)\nclass_weight = dict(enumerate(cw_vals))\n\n#model = build_two_branch_model(pad_len, imu_dim_final, tof_thm_aggregated_dim_final, len(le.classes_), wd=WD)\nmodel = build_transformer_model(input_shape=(pad_len, 47), n_classes=18 )\n\nsteps = len(X_tr) // BATCH_SIZE\nlr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(5e-4, first_decay_steps=15 * steps) \n\nmodel.compile(optimizer=Adam(lr_sched),\n              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n              metrics=['accuracy'])\n\ntrain_gen = MixupGenerator(X_tr, y_tr, batch_size=BATCH_SIZE, alpha=MIXUP_ALPHA)\ncb = EarlyStopping(patience=PATIENCE, restore_best_weights=True, verbose=1, monitor='val_accuracy', mode='max')\n\nprint(\"  Starting model training...\")\nmodel.fit(train_gen, epochs=EPOCHS, validation_data=(X_val, y_val),\n          class_weight=class_weight, callbacks=[cb], verbose=1)\n\nmodel.save(EXPORT_DIR / \"gesture_two_branch_mixup.h5\")\nprint(\"✔ Training done – artefacts saved in\", EXPORT_DIR)\n\nfrom cmi_2025_metric_copy_for_import import CompetitionMetric\npreds_val = model.predict(X_val).argmax(1)\ntrue_val_int  = y_val.argmax(1)\n\nh_f1 = CompetitionMetric().calculate_hierarchical_f1(\n    pd.DataFrame({'gesture': le.classes_[true_val_int]}),\n    pd.DataFrame({'gesture': le.classes_[preds_val]}))\nprint(\"Hold‑out H‑F1 =\", round(h_f1, 4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T21:40:47.282242Z","iopub.execute_input":"2025-07-19T21:40:47.282886Z","iopub.status.idle":"2025-07-19T21:48:28.324363Z","shell.execute_reply.started":"2025-07-19T21:40:47.282855Z","shell.execute_reply":"2025-07-19T21:48:28.323734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === IMU MODEL TRAINING ===\nprint(\"▶ TRAINING IMU MODEL\")\n\nimu_feature_cols = imu_cols  # Already defined from your earlier feature engineering\n\n# Prepare IMU-only sequences\nX_list_imu_unscaled = []\nfor seq_id, seq_df in df.groupby('sequence_id'):\n    imu_data = seq_df[imu_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n    X_list_imu_unscaled.append(imu_data)\n\nprint(\"  Scaling IMU data...\")\nall_imu = np.concatenate(X_list_imu_unscaled, axis=0)\nscaler_imu = StandardScaler().fit(all_imu)\njoblib.dump(scaler_imu, EXPORT_DIR / \"scaler_imu.pkl\")\n\nX_imu_scaled = [scaler_imu.transform(x) for x in X_list_imu_unscaled]\nX_imu = pad_sequences(X_imu_scaled, maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n\nX_tr_imu, X_val_imu, y_tr, y_val = train_test_split(X_imu, y, test_size=0.2, random_state=82, stratify=y_int_for_stratify)\n\ncw_vals = compute_class_weight('balanced', classes=np.arange(len(le.classes_)), y=y_int_for_stratify)\nclass_weight = dict(enumerate(cw_vals))\n\nprint(\"  Building and training IMU model...\")\n#imu_model = build_imu_model(pad_len, imu_dim_final, len(le.classes_), wd=WD)\n#imu_model = build_transformer_model(input_shape=(pad_len, imu_dim_final), n_classes=len(le.classes_))\nimu_model = build_transformer_model(input_shape=(127, 22), n_classes=18 )\n\nsteps = len(X_tr_imu) // BATCH_SIZE\nlr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(5e-4, first_decay_steps=15 * steps) \n\nimu_model.compile(\n    optimizer=Adam(lr_sched),\n    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n    metrics=['accuracy']\n)\n\ntrain_gen_imu = MixupGenerator(X_tr_imu, y_tr, batch_size=BATCH_SIZE, alpha=MIXUP_ALPHA)\ncb = EarlyStopping(patience=PATIENCE, restore_best_weights=True, verbose=1, monitor='val_accuracy', mode='max')\nimu_model.fit(train_gen_imu, epochs=EPOCHS, validation_data=(X_val_imu, y_val),\n              class_weight=class_weight, callbacks=[cb], verbose=1)\n\nimu_model.save(EXPORT_DIR / \"imu_model.h5\")\n\nfrom cmi_2025_metric_copy_for_import import CompetitionMetric\npreds_val = imu_model.predict(X_val_imu).argmax(1)\ntrue_val_int  = y_val.argmax(1)\n\nh_f1 = CompetitionMetric().calculate_hierarchical_f1(\n    pd.DataFrame({'gesture': le.classes_[true_val_int]}),\n    pd.DataFrame({'gesture': le.classes_[preds_val]}))\nprint(\"Hold‑out H‑F1 =\", round(h_f1, 4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T21:29:31.285312Z","iopub.execute_input":"2025-07-19T21:29:31.285912Z","iopub.status.idle":"2025-07-19T21:37:25.189276Z","shell.execute_reply.started":"2025-07-19T21:29:31.285890Z","shell.execute_reply":"2025-07-19T21:37:25.188514Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_tr_imu.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:05:30.717350Z","iopub.execute_input":"2025-07-18T21:05:30.717552Z","iopub.status.idle":"2025-07-18T21:05:30.722393Z","shell.execute_reply.started":"2025-07-18T21:05:30.717536Z","shell.execute_reply":"2025-07-18T21:05:30.721805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pad_len","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T21:05:30.722984Z","iopub.execute_input":"2025-07-18T21:05:30.723236Z","iopub.status.idle":"2025-07-18T21:05:30.736860Z","shell.execute_reply.started":"2025-07-18T21:05:30.723218Z","shell.execute_reply":"2025-07-18T21:05:30.736165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === TOF MODEL TRAINING ===\nprint(\"▶ TRAINING TOF+THM MODEL\")\n\ntof_thm_feature_cols = thm_cols_original + tof_aggregated_cols_template\n\nfor seq_id, seq_df in df.groupby('sequence_id'):\n    for i in range(1, 6):\n        pixel_cols_tof = [f\"tof_{i}_v{p}\" for p in range(64)]\n        tof_sensor_data = seq_df[pixel_cols_tof].replace(-1, np.nan)\n        df.loc[seq_df.index, f'tof_{i}_mean'] = tof_sensor_data.mean(axis=1)\n        df.loc[seq_df.index, f'tof_{i}_std']  = tof_sensor_data.std(axis=1)\n        df.loc[seq_df.index, f'tof_{i}_min']  = tof_sensor_data.min(axis=1)\n        df.loc[seq_df.index, f'tof_{i}_max']  = tof_sensor_data.max(axis=1)\n        \nX_list_tof_unscaled = []\nfor seq_id, seq_df in df.groupby('sequence_id'):\n    tof_data = seq_df[tof_thm_feature_cols].ffill().bfill().fillna(0).values.astype('float32')\n    X_list_tof_unscaled.append(tof_data)\n\n\n\nprint(\"  Scaling TOF+THM data...\")\nall_tof = np.concatenate(X_list_tof_unscaled, axis=0)\nscaler_tof = StandardScaler().fit(all_tof)\njoblib.dump(scaler_tof, EXPORT_DIR / \"scaler_tof.pkl\")\n\nX_tof_scaled = [scaler_tof.transform(x) for x in X_list_tof_unscaled]\nX_tof = pad_sequences(X_tof_scaled, maxlen=pad_len, padding='post', truncating='post', dtype='float32')\n\nX_tr_tof, X_val_tof, y_tr, y_val = train_test_split(X_tof, y, test_size=0.2, random_state=82, stratify=y_int_for_stratify)\n\ncw_vals = compute_class_weight('balanced', classes=np.arange(len(le.classes_)), y=y_int_for_stratify)\nclass_weight = dict(enumerate(cw_vals))\n\nprint(\"  Building and training TOF model...\")\ntof_model = build_tof_model(pad_len, tof_thm_aggregated_dim_final, len(le.classes_), wd=WD)\nsteps = len(X_tr_tof) // BATCH_SIZE\nlr_sched = tf.keras.optimizers.schedules.CosineDecayRestarts(5e-4, first_decay_steps=15 * steps) \n\ntof_model.compile(\n    optimizer=Adam(lr_sched),\n    loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n    metrics=['accuracy']\n)\n\ntrain_gen_tof = MixupGenerator(X_tr_tof, y_tr, batch_size=BATCH_SIZE, alpha=MIXUP_ALPHA)\ncb = EarlyStopping(patience=PATIENCE, restore_best_weights=True, verbose=1, monitor='val_accuracy', mode='max')\ntof_model.fit(train_gen_tof, epochs=EPOCHS, validation_data=(X_val_tof, y_val),\n              class_weight=class_weight, callbacks=[cb], verbose=1)\n\ntof_model.save(EXPORT_DIR / \"tof_model.h5\")\n\nfrom cmi_2025_metric_copy_for_import import CompetitionMetric\npreds_val = tof_model.predict(X_val_tof).argmax(1)\ntrue_val_int  = y_val.argmax(1)\n\nh_f1 = CompetitionMetric().calculate_hierarchical_f1(\n    pd.DataFrame({'gesture': le.classes_[true_val_int]}),\n    pd.DataFrame({'gesture': le.classes_[preds_val]}))\nprint(\"Hold‑out H‑F1 =\", round(h_f1, 4))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-07-13T05:53:07.975Z"}},"outputs":[],"execution_count":null}]}